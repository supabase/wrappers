[AWS S3](https://aws.amazon.com/s3/) is an object storage service offering industry-leading scalability, data availability, security, and performance. The S3 wrapper is under development. It is read-only and supports below file formats:

1. CSV - with or without header line
2. [JSON Lines](https://jsonlines.org/)
3. [Parquet](https://parquet.apache.org/)

The S3 FDW also supports below compression algorithms:

1. gzip
2. bzip2
3. xz
4. zlib

**Note for CSV and JSONL files: currently all columns in S3 files must be defined in the foreign table and their types must be `text` type**

**Note for Parquet files: the whole Parquet file will be loaded into local memory if it is compressed, so keep its size small**

### Supported Data Types For Parquet File

The S3 FDW uses Parquet file data types from [arrow_array::types](https://docs.rs/arrow-array/41.0.0/arrow_array/types/index.html), below are their mappings to Postgres data types.

| Postgres Type      | Parquet Type             |
| ------------------ | ------------------------ |
| boolean            | BooleanType              |
| char               | Int8Type                 |
| smallint           | Int16Type                |
| real               | Float32Type              |
| integer            | Int32Type                |
| double precision   | Float64Type              |
| bigint             | Int64Type                |
| numeric            | Float64Type              |
| text               | ByteArrayType            |
| date               | Date64Type               |
| timestamp          | TimestampNanosecondType  |

### Wrapper 
To get started with the S3 wrapper, create a foreign data wrapper specifying `handler` and `validator` as below.

```sql
create extension if not exists wrappers;

create foreign data wrapper s3_wrapper
  handler s3_fdw_handler
  validator s3_fdw_validator;
```

### Server 

Next, we need to create a server for the FDW to hold options and credentials.

#### Auth (Supabase)

If you are using the Supabase platform, this is the recommended approach for securing your AWS credentials.

For example, to store AWS access key id to `vault_access_key_id` in Vault:

Create a secure key using pgsodium
```sql
select pgsodium.create_key(name := 'vault_access_key_id');
```

Save your credential in Vault and retrieve the `key_id`
```sql
insert into
  vault.secrets (secret, key_id)
values 
  (
    'AKIAXXXXXX',
    (select id from pgsodium.valid_key where name = 'vault_access_key_id')
  )
returning
  key_id;
```

You can follow the same steps to store `vault_secret_access_key` in Vault.

Then create the foreign server

```sql
create server s3_server
  foreign data wrapper s3_wrapper
  options (
    vault_access_key_id '<your vault_access_key_id from above>',
    vault_secret_access_key '<your vault_secret_access_key_id from above>',
    aws_region 'us-east-1'
  );
```

#### Auth (Non-Supabase)

If the platform you are using does not support `pgsodium` and `Vault`, you can create a server by storing your AWS credentials directly.


!!! important

    Credentials stored using this method can be viewed as plain text by anyone with access to `pg_catalog.pg_foreign_server`


```sql
create server s3_server
  foreign data wrapper s3_wrapper
   options (
    aws_access_key_id 'your_aws_access_key_id',
    aws_secret_access_key 'your_aws_secret_access_key',
    aws_region 'us-east-1'
   );
```


### Tables

S3 wrapper is implemented with [ELT](https://hevodata.com/learn/etl-vs-elt/) approach, so the data transformation is encouraged to be performed locally after data is extracted from remote data source.

One file in S3 corresponds a foreign table in Postgres. For CSV and JSONL file, all columns must be present in the foreign table and type must be `text`. You can do custom transformations, like type conversion, by creating a view on top of the foreign table or using a subquery.

For Parquet file, no need to define all columns in the foreign table but column names must match between Parquet file and its foreign table.


#### Foreign Table Options

The full list of foreign table options are below:

- `uri` - S3 URI, required. For example, `s3://bucket/s3_table.csv`
- `format` - File format, required. `csv`, `jsonl`, or `parquet`
- `has_header` - If the CSV file has header, optional. `true` or `false`, default is `false`
- `compress` - Compression algorithm, optional. One of `gzip`, `bzip2`, `xz`, `zlib`, default is no compression

#### Examples

```sql
-- CSV file, no compression
create foreign table s3_table_csv (
  name text,
  sex text,
  age text,
  height text,
  weight text
)
  server s3_server
  options (
    uri 's3://bucket/s3_table.csv',
    format 'csv',
    has_header 'true'
  );

-- JSON line file, no compression
create foreign table s3_table_jsonl (
  name text,
  sex text,
  age text,
  height text,
  weight text
)
  server s3_server
  options (
    uri 's3://bucket/s3_table.jsonl',
    format 'jsonl'
  );

-- GZIP compressed CSV file
create foreign table s3_table_csv_gzip (
  name text,
  sex text,
  age text,
  height text,
  weight text
)
  server s3_server
  options (
    uri 's3://bucket/s3_table.csv.gz',
    format 'csv',
    has_header 'true',
    compress 'gzip'
  );

-- Parquet file, no compression
create foreign table s3_table_parquet (
  id integer,
  bool_col boolean,
  bigint_col bigint,
  float_col real,
  date_string_col text,
  timestamp_col timestamp
)
  server s3_server
  options (
    uri 's3://bucket/s3_table.parquet',
    format 'parquet'
  );

-- GZIP compressed Parquet file
create foreign table s3_table_parquet_gz (
  id integer,
  bool_col boolean,
  bigint_col bigint,
  float_col real,
  date_string_col text,
  timestamp_col timestamp
)
  server s3_server
  options (
    uri 's3://bucket/s3_table.parquet.gz',
    format 'parquet',
    compress 'gzip'
  );
```

